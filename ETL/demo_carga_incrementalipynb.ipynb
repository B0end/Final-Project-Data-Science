{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23cd5f-7bc0-4c6f-8039-cc20f54bf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "\n",
    "credentials = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"cedar-pottery-388916\",\n",
    "  \"private_key_id\": \"SECRET\",\n",
    "  \"private_key\": \"SECRET\",\n",
    "  \"client_email\": \"drive-895@cedar-pottery-388916.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"113489151660006131618\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/drive-895%40cedar-pottery-388916.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_info(credentials)\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Define the project ID\n",
    "project_id = 'cedar-pottery-388916'\n",
    "\n",
    "# Define the destination bucket name in Google Cloud Storage\n",
    "bucket_name = 'bucket123321'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495bb3bc-c3de-4fe4-9d05-62161845f319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Name\": \"12.json\",\n",
      "        \"ID\": \"1Of9OzNb9tSoghB2sk3rS5OoLjtia2BzX\",\n",
      "        \"Last Modified Date\": \"2023-06-15T23:43:55.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"11.json\",\n",
      "        \"ID\": \"1ZQxtdJSx9Pgu0XM2Zhn8Jts4Mw6J1x9i\",\n",
      "        \"Last Modified Date\": \"2023-05-29T20:53:25.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"10.json\",\n",
      "        \"ID\": \"1UjTwRPYVlx_tqFOPCcRr9WSmNNtSWgnb\",\n",
      "        \"Last Modified Date\": \"2023-05-29T20:37:08.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"7.json\",\n",
      "        \"ID\": \"1RRbDaqhj7mUE63FeBNIZnMr2I6tsdpDl\",\n",
      "        \"Last Modified Date\": \"2023-05-29T20:29:40.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"9.json\",\n",
      "        \"ID\": \"1QjRIUj3UxPAC0gLXmEo21gu_OebwiqBX\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:42:56.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"8.json\",\n",
      "        \"ID\": \"1dYAvdjpLV9GZFejB2hbsIsNjr0-LKI2b\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:42:42.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"6.json\",\n",
      "        \"ID\": \"1-FRQ8PgfXal_CgEUDlFNpi4s1X2Ylykt\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:42:18.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"5.json\",\n",
      "        \"ID\": \"1CgzDqnbNuPhTNQT6ORLzYTCxlVKiwIVF\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:42:08.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"4.json\",\n",
      "        \"ID\": \"1JOowm6HMeZT4PoHBu2xiSfHfDoWnG1z7\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:41:56.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"3.json\",\n",
      "        \"ID\": \"1N89VDxNI5HkoR-001wIx1IeoWJ_wHpgu\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:41:42.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"2.json\",\n",
      "        \"ID\": \"1lGfnJOkwFmF31f1p9pNWRToyr4owOOwe\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:41:32.000Z\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"1.json\",\n",
      "        \"ID\": \"1EN0p4oUggO4qc8wrZ7cDzPQHgVU0tgXF\",\n",
      "        \"Last Modified Date\": \"2023-02-03T02:41:18.000Z\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve directory and file information recursively\n",
    "def get_directory_and_file_info(service, directory_id, parent_directory_name=\"\"):\n",
    "    file_info_list = []\n",
    "\n",
    "    # Query for files in the current directory\n",
    "    query = f\"'{directory_id}' in parents\"\n",
    "    response = service.files().list(q=query, fields='files(id, name, mimeType, modifiedTime), nextPageToken').execute()\n",
    "\n",
    "    # Iterate over the files in the current directory\n",
    "    for file in response.get('files', []):\n",
    "        if file.get('mimeType') == 'application/vnd.google-apps.folder':\n",
    "            # If the file is a subdirectory, recursively call the function\n",
    "            directory_name = parent_directory_name + file['name'] + \"/\"\n",
    "            file_info_list.extend(get_directory_and_file_info(service, file['id'], directory_name))\n",
    "        else:\n",
    "            # If the file is not a subdirectory, add the file information to the list\n",
    "            file_info = {\n",
    "                \"Name\": parent_directory_name + file['name'],\n",
    "                \"ID\": file['id'],\n",
    "                \"Last Modified Date\": file.get('modifiedTime', '')\n",
    "            }\n",
    "            file_info_list.append(file_info)\n",
    "\n",
    "    return file_info_list\n",
    "\n",
    "# ID of the top-level directory\n",
    "top_directory_id = '1wGNQYA3f_PJpUkarEes0txvYpQ2k1ctB'\n",
    "\n",
    "# Retrieve directory and file information recursively\n",
    "file_info_list = get_directory_and_file_info(drive_service, top_directory_id)\n",
    "\n",
    "# Convert the file information list to JSON string\n",
    "file_info_json = json.dumps(file_info_list, indent=4)\n",
    "\n",
    "# Print the file information in JSON-like format\n",
    "print(file_info_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d84796b-5441-4f84-b1a2-073a8476a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "The GCS file is different from the result from the code.\n",
      "-----------------------------------------------------\n",
      "[\n",
      "    {\n",
      "        \"Name\": \"11.json\",\n",
      "        \"ID\": \"1ZQxtdJSx9Pgu0XM2Zhn8Jts4Mw6J1x9i\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"10.json\",\n",
      "        \"ID\": \"1UjTwRPYVlx_tqFOPCcRr9WSmNNtSWgnb\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"7.json\",\n",
      "        \"ID\": \"1RRbDaqhj7mUE63FeBNIZnMr2I6tsdpDl\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"9.json\",\n",
      "        \"ID\": \"1QjRIUj3UxPAC0gLXmEo21gu_OebwiqBX\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"8.json\",\n",
      "        \"ID\": \"1dYAvdjpLV9GZFejB2hbsIsNjr0-LKI2b\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"6.json\",\n",
      "        \"ID\": \"1-FRQ8PgfXal_CgEUDlFNpi4s1X2Ylykt\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"5.json\",\n",
      "        \"ID\": \"1CgzDqnbNuPhTNQT6ORLzYTCxlVKiwIVF\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"4.json\",\n",
      "        \"ID\": \"1JOowm6HMeZT4PoHBu2xiSfHfDoWnG1z7\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"3.json\",\n",
      "        \"ID\": \"1N89VDxNI5HkoR-001wIx1IeoWJ_wHpgu\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"2.json\",\n",
      "        \"ID\": \"1lGfnJOkwFmF31f1p9pNWRToyr4owOOwe\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"1.json\",\n",
      "        \"ID\": \"1EN0p4oUggO4qc8wrZ7cDzPQHgVU0tgXF\",\n",
      "        \"Last Modified Date\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Read the content of the file in GCS\n",
    "def read_file_from_gcs(file_path):\n",
    "    # Instantiate a client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket and blob names from the file path\n",
    "    bucket_name, blob_name = file_path.split(\"/\", 3)[2:]\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Get the blob\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Download the file contents\n",
    "    content = blob.download_as_text()\n",
    "    return content\n",
    "\n",
    "# Compare the file in GCS with the result from the code above\n",
    "def compare_file_with_result(gcs_file_path, result):\n",
    "    # Read the content of the file in GCS\n",
    "    gcs_content = read_file_from_gcs(gcs_file_path)\n",
    "\n",
    "    # Parse the JSON content of the GCS file\n",
    "    gcs_data = json.loads(gcs_content)\n",
    "\n",
    "    # Compare the results\n",
    "    if len(gcs_data) != len(result):\n",
    "        return False\n",
    "\n",
    "    for gcs_file, code_file in zip(gcs_data, result):\n",
    "        if gcs_file != code_file:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "gcs_registers_file_path = \"gs://bucket123321/incremental_load/registers.json\"\n",
    "\n",
    "# Assuming you have the 'result' variable containing the result from the code above\n",
    "comparison_result = compare_file_with_result(gcs_registers_file_path, file_info_list)\n",
    "\n",
    "# Print the result\n",
    "if comparison_result:\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"The GCS file is the same as the result from the code.\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "else:\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"The GCS file is different from the result from the code.\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "content = read_file_from_gcs(gcs_registers_file_path)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6590723-f1b9-4c1e-90db-017cf24bee26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the JSON string into a list of dictionaries\n",
    "file_info_json = json.loads(file_info_json)\n",
    "content = json.loads(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876f370d-b1e9-48a2-adbd-3338de5c8f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Name': '12.json', 'ID': '1Of9OzNb9tSoghB2sk3rS5OoLjtia2BzX', 'Last Modified Date': '2023-06-15T23:43:55.000Z'}]\n"
     ]
    }
   ],
   "source": [
    "new_elements = []\n",
    "file_names = [element['Name'] for element in content]\n",
    "for element in file_info_json:\n",
    "    if element['Name'] not in file_names:\n",
    "        new_elements.append(element)\n",
    "print(new_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd58e6ac-7b6d-4670-ab7d-05eb80a85b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1Of9OzNb9tSoghB2sk3rS5OoLjtia2BzX'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_elements[0]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d25baa-c51c-4645-b7ac-58330779a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File transfer completed for file ID: <built-in function id>\n"
     ]
    }
   ],
   "source": [
    "# Create a client for Google Cloud Storage directly using the credentials\n",
    "client = storage.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Define the target directory path within the bucket\n",
    "target_directory = \"gmap_metadata/new_data/\"\n",
    "\n",
    "# Iterate over the file URLs and names simultaneously\n",
    "for element in new_elements:\n",
    "\n",
    "    # Download the file from Google Drive\n",
    "    request = drive_service.files().get_media(fileId=element['ID'])\n",
    "    file_stream = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_stream, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "\n",
    "    # Upload the file to Google Cloud Storage\n",
    "    target_blob_name = target_directory + element['Name']\n",
    "    blob = client.bucket(bucket_name).blob(target_blob_name)\n",
    "    file_stream.seek(0)  # Reset the stream position to the beginning\n",
    "    blob.upload_from_file(file_stream, content_type='application/octet-stream')\n",
    "\n",
    "    print('File transfer completed for file ID:', id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0019aca8-7dba-4a14-9e32-7d44d3813bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------------------+--------+---------+-----------+--------------+--------------------+--------------------+\n",
      "|             address|avg_rating|category|             gmap_id|latitude|longitude|       name|num_of_reviews|    relative_results|                 url|\n",
      "+--------------------+----------+--------+--------------------+--------+---------+-----------+--------------+--------------------+--------------------+\n",
      "|742 Evergreen Ter...|       4.9| [House]|0x88f1deddd28ff68...| 32.3783| -83.3511|Grand Place|            16|[0x88f16e41929435...|https://www.googl...|\n",
      "+--------------------+----------+--------+--------------------+--------+---------+-----------+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the JSON files into Spark DataFrames\n",
    "gmaps_metadata = spark.read.json('gs://bucket123321/gmap_metadata/new_data/*.json')\n",
    "\n",
    "# Drop columns\n",
    "columns_to_drop = [\"hours\", \"MISC\", \"state\", \"price\", \"description\"]\n",
    "gmaps_metadata = gmaps_metadata.drop(*columns_to_drop)\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "gmaps_metadata = gmaps_metadata.dropDuplicates()\n",
    "\n",
    "gmaps_metadata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f7ed4e-8c28-43ce-964d-2afcf882ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the output JSON file path\n",
    "output_json_path = 'gs://bucket123321/gmap_clean_data/metadata/new_data'\n",
    "\n",
    "# Repartition the DataFrame to a single partition\n",
    "gmaps_metadata = gmaps_metadata.repartition(1)\n",
    "\n",
    "# Write the DataFrame to a JSON file \n",
    "gmaps_metadata.write.json(output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601240af-e9f3-42ee-b97a-d82ba251650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r481d05d9984ba213_00000188c2051bb2_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq load --autodetect --source_format=NEWLINE_DELIMITED_JSON conjunto_testing.gmaps_metadata_testing gs://bucket123321/gmap_clean_data/metadata/new_data/*.json\n",
    "\n",
    "# Initialize the client\n",
    "client = storage.Client()\n",
    "\n",
    "# Specify the GCS bucket and JSON file path\n",
    "bucket_name = \"bucket123321\"\n",
    "json_file_path = \"incremental_load/registers.json\"\n",
    "\n",
    "# Convert the file_info_json variable to JSON format\n",
    "json_data = json.dumps(file_info_json)\n",
    "\n",
    "# Upload the JSON data to GCS, replacing the existing JSON file\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(json_file_path)\n",
    "blob.upload_from_string(json_data, content_type=\"application/json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
